{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Data Science Academy - Machine Learning</font>\n",
    "\n",
    "# <font color='blue'>Capítulo 4 - Regressão Linear Múltipla</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando o Dataset Boston Houses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. CRIM: per capita crime rate by town \n",
    "2. ZN: proportion of residential land zoned for lots over 25,000 sq.ft. \n",
    "3. INDUS: proportion of non-residential acres per town \n",
    "4. CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) \n",
    "5. NOX: nitric oxides concentration (parts per 10 million) \n",
    "6. RM: average number of rooms per dwelling \n",
    "7. AGE: proportion of owner-occupied units built prior to 1940 \n",
    "8. DIS: weighted distances to five Boston employment centres \n",
    "9. RAD: index of accessibility to radial highways \n",
    "10. TAX: full-value property-tax rate per 10,000 \n",
    "11. PTRATIO: pupil-teacher ratio by town \n",
    "12. B: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town \n",
    "13. LSTAT: % lower status of the population \n",
    "14. TARGET: Median value of owner-occupied homes in $1000's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerando o dataset\n",
    "boston = load_boston() \n",
    "dataset = pd.DataFrame(boston.data, columns = boston.feature_names)\n",
    "dataset['target'] = boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerando número de observações e variáveis\n",
    "observations = len(dataset)\n",
    "variables = dataset.columns[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coletando x e y\n",
    "X = dataset.iloc[:,:-1]\n",
    "y = dataset['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  \n",
       "0     15.3  396.90   4.98  \n",
       "1     17.8  396.90   9.14  \n",
       "2     17.8  392.83   4.03  \n",
       "3     18.7  394.63   2.94  \n",
       "4     18.7  396.90   5.33  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24. , 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15. ,\n",
       "       18.9, 21.7, 20.4, 18.2, 19.9, 23.1, 17.5, 20.2, 18.2, 13.6, 19.6,\n",
       "       15.2, 14.5, 15.6, 13.9, 16.6, 14.8, 18.4, 21. , 12.7, 14.5, 13.2,\n",
       "       13.1, 13.5, 18.9, 20. , 21. , 24.7, 30.8, 34.9, 26.6, 25.3, 24.7,\n",
       "       21.2, 19.3, 20. , 16.6, 14.4, 19.4, 19.7, 20.5, 25. , 23.4, 18.9,\n",
       "       35.4, 24.7, 31.6, 23.3, 19.6, 18.7, 16. , 22.2, 25. , 33. , 23.5,\n",
       "       19.4, 22. , 17.4, 20.9, 24.2, 21.7, 22.8, 23.4, 24.1, 21.4, 20. ,\n",
       "       20.8, 21.2, 20.3, 28. , 23.9, 24.8, 22.9, 23.9, 26.6, 22.5, 22.2,\n",
       "       23.6, 28.7, 22.6, 22. , 22.9, 25. , 20.6, 28.4, 21.4, 38.7, 43.8,\n",
       "       33.2, 27.5, 26.5, 18.6, 19.3, 20.1, 19.5, 19.5, 20.4, 19.8, 19.4,\n",
       "       21.7, 22.8, 18.8, 18.7, 18.5, 18.3, 21.2, 19.2, 20.4, 19.3, 22. ,\n",
       "       20.3, 20.5, 17.3, 18.8, 21.4, 15.7, 16.2, 18. , 14.3, 19.2, 19.6,\n",
       "       23. , 18.4, 15.6, 18.1, 17.4, 17.1, 13.3, 17.8, 14. , 14.4, 13.4,\n",
       "       15.6, 11.8, 13.8, 15.6, 14.6, 17.8, 15.4, 21.5, 19.6, 15.3, 19.4,\n",
       "       17. , 15.6, 13.1, 41.3, 24.3, 23.3, 27. , 50. , 50. , 50. , 22.7,\n",
       "       25. , 50. , 23.8, 23.8, 22.3, 17.4, 19.1, 23.1, 23.6, 22.6, 29.4,\n",
       "       23.2, 24.6, 29.9, 37.2, 39.8, 36.2, 37.9, 32.5, 26.4, 29.6, 50. ,\n",
       "       32. , 29.8, 34.9, 37. , 30.5, 36.4, 31.1, 29.1, 50. , 33.3, 30.3,\n",
       "       34.6, 34.9, 32.9, 24.1, 42.3, 48.5, 50. , 22.6, 24.4, 22.5, 24.4,\n",
       "       20. , 21.7, 19.3, 22.4, 28.1, 23.7, 25. , 23.3, 28.7, 21.5, 23. ,\n",
       "       26.7, 21.7, 27.5, 30.1, 44.8, 50. , 37.6, 31.6, 46.7, 31.5, 24.3,\n",
       "       31.7, 41.7, 48.3, 29. , 24. , 25.1, 31.5, 23.7, 23.3, 22. , 20.1,\n",
       "       22.2, 23.7, 17.6, 18.5, 24.3, 20.5, 24.5, 26.2, 24.4, 24.8, 29.6,\n",
       "       42.8, 21.9, 20.9, 44. , 50. , 36. , 30.1, 33.8, 43.1, 48.8, 31. ,\n",
       "       36.5, 22.8, 30.7, 50. , 43.5, 20.7, 21.1, 25.2, 24.4, 35.2, 32.4,\n",
       "       32. , 33.2, 33.1, 29.1, 35.1, 45.4, 35.4, 46. , 50. , 32.2, 22. ,\n",
       "       20.1, 23.2, 22.3, 24.8, 28.5, 37.3, 27.9, 23.9, 21.7, 28.6, 27.1,\n",
       "       20.3, 22.5, 29. , 24.8, 22. , 26.4, 33.1, 36.1, 28.4, 33.4, 28.2,\n",
       "       22.8, 20.3, 16.1, 22.1, 19.4, 21.6, 23.8, 16.2, 17.8, 19.8, 23.1,\n",
       "       21. , 23.8, 23.1, 20.4, 18.5, 25. , 24.6, 23. , 22.2, 19.3, 22.6,\n",
       "       19.8, 17.1, 19.4, 22.2, 20.7, 21.1, 19.5, 18.5, 20.6, 19. , 18.7,\n",
       "       32.7, 16.5, 23.9, 31.2, 17.5, 17.2, 23.1, 24.5, 26.6, 22.9, 24.1,\n",
       "       18.6, 30.1, 18.2, 20.6, 17.8, 21.7, 22.7, 22.6, 25. , 19.9, 20.8,\n",
       "       16.8, 21.9, 27.5, 21.9, 23.1, 50. , 50. , 50. , 50. , 50. , 13.8,\n",
       "       13.8, 15. , 13.9, 13.3, 13.1, 10.2, 10.4, 10.9, 11.3, 12.3,  8.8,\n",
       "        7.2, 10.5,  7.4, 10.2, 11.5, 15.1, 23.2,  9.7, 13.8, 12.7, 13.1,\n",
       "       12.5,  8.5,  5. ,  6.3,  5.6,  7.2, 12.1,  8.3,  8.5,  5. , 11.9,\n",
       "       27.9, 17.2, 27.5, 15. , 17.2, 17.9, 16.3,  7. ,  7.2,  7.5, 10.4,\n",
       "        8.8,  8.4, 16.7, 14.2, 20.8, 13.4, 11.7,  8.3, 10.2, 10.9, 11. ,\n",
       "        9.5, 14.5, 14.1, 16.1, 14.3, 11.7, 13.4,  9.6,  8.7,  8.4, 12.8,\n",
       "       10.5, 17.1, 18.4, 15.4, 10.8, 11.8, 14.9, 12.6, 14.1, 13. , 13.4,\n",
       "       15.2, 16.1, 17.8, 14.9, 14.1, 12.7, 13.5, 14.9, 20. , 16.4, 17.7,\n",
       "       19.5, 20.2, 21.4, 19.9, 19. , 19.1, 19.1, 20.1, 19.9, 19.6, 23.2,\n",
       "       29.8, 13.8, 13.3, 16.7, 12. , 14.6, 21.4, 23. , 23.7, 25. , 21.8,\n",
       "       20.6, 21.2, 19.1, 20.6, 15.2,  7. ,  8.1, 13.6, 20.1, 21.8, 24.5,\n",
       "       23.1, 19.7, 18.3, 21.2, 17.5, 16.8, 22.4, 20.6, 23.9, 22. , 11.9])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando Múltiplos Atributos com StatsModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xc = sm.add_constant(X)\n",
    "modelo_v1 = sm.OLS(y, Xc)\n",
    "modelo_v2 = modelo_v1.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.741</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.734</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   108.1</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 24 Jul 2018</td> <th>  Prob (F-statistic):</th> <td>6.95e-135</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>20:27:42</td>     <th>  Log-Likelihood:    </th> <td> -1498.8</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   506</td>      <th>  AIC:               </th> <td>   3026.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   492</td>      <th>  BIC:               </th> <td>   3085.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    13</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>        <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>   <td>   36.4911</td> <td>    5.104</td> <td>    7.149</td> <td> 0.000</td> <td>   26.462</td> <td>   46.520</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CRIM</th>    <td>   -0.1072</td> <td>    0.033</td> <td>   -3.276</td> <td> 0.001</td> <td>   -0.171</td> <td>   -0.043</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ZN</th>      <td>    0.0464</td> <td>    0.014</td> <td>    3.380</td> <td> 0.001</td> <td>    0.019</td> <td>    0.073</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>INDUS</th>   <td>    0.0209</td> <td>    0.061</td> <td>    0.339</td> <td> 0.735</td> <td>   -0.100</td> <td>    0.142</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CHAS</th>    <td>    2.6886</td> <td>    0.862</td> <td>    3.120</td> <td> 0.002</td> <td>    0.996</td> <td>    4.381</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>NOX</th>     <td>  -17.7958</td> <td>    3.821</td> <td>   -4.658</td> <td> 0.000</td> <td>  -25.302</td> <td>  -10.289</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RM</th>      <td>    3.8048</td> <td>    0.418</td> <td>    9.102</td> <td> 0.000</td> <td>    2.983</td> <td>    4.626</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AGE</th>     <td>    0.0008</td> <td>    0.013</td> <td>    0.057</td> <td> 0.955</td> <td>   -0.025</td> <td>    0.027</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>DIS</th>     <td>   -1.4758</td> <td>    0.199</td> <td>   -7.398</td> <td> 0.000</td> <td>   -1.868</td> <td>   -1.084</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RAD</th>     <td>    0.3057</td> <td>    0.066</td> <td>    4.608</td> <td> 0.000</td> <td>    0.175</td> <td>    0.436</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>TAX</th>     <td>   -0.0123</td> <td>    0.004</td> <td>   -3.278</td> <td> 0.001</td> <td>   -0.020</td> <td>   -0.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>PTRATIO</th> <td>   -0.9535</td> <td>    0.131</td> <td>   -7.287</td> <td> 0.000</td> <td>   -1.211</td> <td>   -0.696</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>B</th>       <td>    0.0094</td> <td>    0.003</td> <td>    3.500</td> <td> 0.001</td> <td>    0.004</td> <td>    0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LSTAT</th>   <td>   -0.5255</td> <td>    0.051</td> <td>  -10.366</td> <td> 0.000</td> <td>   -0.625</td> <td>   -0.426</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>178.029</td> <th>  Durbin-Watson:     </th> <td>   1.078</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 782.015</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.521</td>  <th>  Prob(JB):          </th> <td>1.54e-170</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 8.276</td>  <th>  Cond. No.          </th> <td>1.51e+04</td> \n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.51e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.741\n",
       "Model:                            OLS   Adj. R-squared:                  0.734\n",
       "Method:                 Least Squares   F-statistic:                     108.1\n",
       "Date:                Tue, 24 Jul 2018   Prob (F-statistic):          6.95e-135\n",
       "Time:                        20:27:42   Log-Likelihood:                -1498.8\n",
       "No. Observations:                 506   AIC:                             3026.\n",
       "Df Residuals:                     492   BIC:                             3085.\n",
       "Df Model:                          13                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         36.4911      5.104      7.149      0.000      26.462      46.520\n",
       "CRIM          -0.1072      0.033     -3.276      0.001      -0.171      -0.043\n",
       "ZN             0.0464      0.014      3.380      0.001       0.019       0.073\n",
       "INDUS          0.0209      0.061      0.339      0.735      -0.100       0.142\n",
       "CHAS           2.6886      0.862      3.120      0.002       0.996       4.381\n",
       "NOX          -17.7958      3.821     -4.658      0.000     -25.302     -10.289\n",
       "RM             3.8048      0.418      9.102      0.000       2.983       4.626\n",
       "AGE            0.0008      0.013      0.057      0.955      -0.025       0.027\n",
       "DIS           -1.4758      0.199     -7.398      0.000      -1.868      -1.084\n",
       "RAD            0.3057      0.066      4.608      0.000       0.175       0.436\n",
       "TAX           -0.0123      0.004     -3.278      0.001      -0.020      -0.005\n",
       "PTRATIO       -0.9535      0.131     -7.287      0.000      -1.211      -0.696\n",
       "B              0.0094      0.003      3.500      0.001       0.004       0.015\n",
       "LSTAT         -0.5255      0.051    -10.366      0.000      -0.625      -0.426\n",
       "==============================================================================\n",
       "Omnibus:                      178.029   Durbin-Watson:                   1.078\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              782.015\n",
       "Skew:                           1.521   Prob(JB):                    1.54e-170\n",
       "Kurtosis:                       8.276   Cond. No.                     1.51e+04\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.51e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo_v2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matriz de Correlação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             CRIM        ZN     INDUS      CHAS       NOX        RM       AGE  \\\n",
      "CRIM     1.000000 -0.199458  0.404471 -0.055295  0.417521 -0.219940  0.350784   \n",
      "ZN      -0.199458  1.000000 -0.533828 -0.042697 -0.516604  0.311991 -0.569537   \n",
      "INDUS    0.404471 -0.533828  1.000000  0.062938  0.763651 -0.391676  0.644779   \n",
      "CHAS    -0.055295 -0.042697  0.062938  1.000000  0.091203  0.091251  0.086518   \n",
      "NOX      0.417521 -0.516604  0.763651  0.091203  1.000000 -0.302188  0.731470   \n",
      "RM      -0.219940  0.311991 -0.391676  0.091251 -0.302188  1.000000 -0.240265   \n",
      "AGE      0.350784 -0.569537  0.644779  0.086518  0.731470 -0.240265  1.000000   \n",
      "DIS     -0.377904  0.664408 -0.708027 -0.099176 -0.769230  0.205246 -0.747881   \n",
      "RAD      0.622029 -0.311948  0.595129 -0.007368  0.611441 -0.209847  0.456022   \n",
      "TAX      0.579564 -0.314563  0.720760 -0.035587  0.668023 -0.292048  0.506456   \n",
      "PTRATIO  0.288250 -0.391679  0.383248 -0.121515  0.188933 -0.355501  0.261515   \n",
      "B       -0.377365  0.175520 -0.356977  0.048788 -0.380051  0.128069 -0.273534   \n",
      "LSTAT    0.452220 -0.412995  0.603800 -0.053929  0.590879 -0.613808  0.602339   \n",
      "\n",
      "              DIS       RAD       TAX   PTRATIO         B     LSTAT  \n",
      "CRIM    -0.377904  0.622029  0.579564  0.288250 -0.377365  0.452220  \n",
      "ZN       0.664408 -0.311948 -0.314563 -0.391679  0.175520 -0.412995  \n",
      "INDUS   -0.708027  0.595129  0.720760  0.383248 -0.356977  0.603800  \n",
      "CHAS    -0.099176 -0.007368 -0.035587 -0.121515  0.048788 -0.053929  \n",
      "NOX     -0.769230  0.611441  0.668023  0.188933 -0.380051  0.590879  \n",
      "RM       0.205246 -0.209847 -0.292048 -0.355501  0.128069 -0.613808  \n",
      "AGE     -0.747881  0.456022  0.506456  0.261515 -0.273534  0.602339  \n",
      "DIS      1.000000 -0.494588 -0.534432 -0.232471  0.291512 -0.496996  \n",
      "RAD     -0.494588  1.000000  0.910228  0.464741 -0.444413  0.488676  \n",
      "TAX     -0.534432  0.910228  1.000000  0.460853 -0.441808  0.543993  \n",
      "PTRATIO -0.232471  0.464741  0.460853  1.000000 -0.177383  0.374044  \n",
      "B        0.291512 -0.444413 -0.441808 -0.177383  1.000000 -0.366087  \n",
      "LSTAT   -0.496996  0.488676  0.543993  0.374044 -0.366087  1.000000  \n"
     ]
    }
   ],
   "source": [
    "# Gerando a matriz\n",
    "X = dataset.iloc[:,:-1]\n",
    "matriz_corr = X.corr()\n",
    "print (matriz_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um Correlation Plot\n",
    "def visualize_correlation_matrix(data, hurdle = 0.0):\n",
    "    R = np.corrcoef(data, rowvar = 0)\n",
    "    R[np.where(np.abs(R) < hurdle)] = 0.0\n",
    "    heatmap = plt.pcolor(R, cmap = mpl.cm.coolwarm, alpha = 0.8)\n",
    "    heatmap.axes.set_frame_on(False)\n",
    "    heatmap.axes.set_yticks(np.arange(R.shape[0]) + 0.5, minor = False)\n",
    "    heatmap.axes.set_xticks(np.arange(R.shape[1]) + 0.5, minor = False)\n",
    "    heatmap.axes.set_xticklabels(variables, minor = False)\n",
    "    plt.xticks(rotation=90)\n",
    "    heatmap.axes.set_yticklabels(variables, minor = False)\n",
    "    plt.tick_params(axis = 'both', which = 'both', bottom = 'off', top = 'off', left = 'off', right = 'off') \n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizando o Plot\n",
    "visualize_correlation_matrix(X, hurdle = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliando a Multicolinearidade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autovalores (Eigenvalues) e Autovetores (Eigenvectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma forma ainda mais automática de detectar associações multicolineares (e descobrir problemas numéricos em uma inversão de matriz) é usar autovetores. Explicados em termos simples, os autovetores são uma maneira muito inteligente de recombinar a variância entre as variáveis, criando novos recursos acumulando toda a variância compartilhada. Tal recombinação pode ser obtida usando a função NumPy linalg.eig, resultando em um vetor de autovalores (representando a quantidade de variância recombinada para cada nova variável) e autovetores (uma matriz nos dizendo como as novas variáveis se relacionam com as antigas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerando eigenvalues e eigenvectors\n",
    "corr = np.corrcoef(X, rowvar = 0)\n",
    "eigenvalues, eigenvectors = np.linalg.eig(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois de extrair os autovalores, imprimimos em ordem decrescente e procuramos qualquer elemento cujo valor seja próximo de zero ou pequeno em comparação com os outros. Valores próximos a zero podem representar um problema real para equações normais e outros métodos de otimização baseados na inversão matricial. Valores pequenos representam uma fonte elevada, mas não crítica, de multicolinearidade. Se você detectar qualquer um desses valores baixos, anote a posição no vetor (lembre que os índices em Python começam por zero). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O menor valor está na posição 8. Valor buscar a posição 8 no autovetor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.12265476 1.43206335 1.24116299 0.85779892 0.83456618 0.65965056\n",
      " 0.53901749 0.39654415 0.06351553 0.27743495 0.16916744 0.18616388\n",
      " 0.22025981]\n"
     ]
    }
   ],
   "source": [
    "print (eigenvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando a posição do índice na lista de autovalores, podemos encontrar o vetor específico nos autovetores que contém as variáveis carregadas, ou seja, o nível de associação com os valores originais. No eigenvector, observamos valores nas posições de índice 2, 8 e 9, que estão realmente em destaque em termos de valor absoluto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.04552843  0.08089873  0.25126664 -0.03590431 -0.04389033 -0.04580522\n",
      "  0.03870705  0.01828389  0.63337285 -0.72024335 -0.02350903  0.00485021\n",
      " -0.02477196]\n"
     ]
    }
   ],
   "source": [
    "print (eigenvectors[:,8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora nós imprimimos os nomes das variáveis para saber quais contribuem mais com seus valores para construir o autovetor. Associamos o vetor de variáveis com o eigenvector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDUS RAD TAX\n"
     ]
    }
   ],
   "source": [
    "print (variables[2], variables[8], variables[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tendo encontrado os culpados da multicolinearidade, o que devemos fazer com essas variáveis? A remoção de algumas delas é geralmente a melhor solução."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradiente Descendente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerando os dados\n",
    "observations = len(dataset)\n",
    "variables = dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos aplicar Feature Scaling através de Padronização ou Normalização. Normalização aplica escala aos dados com intervalos entre 0 e 1. A Padronização divide a média pelo desvio padrão para obter uma unidade de variância. Vamos usar a Padronização (StandardScaler) pois nesse caso esta técnica ajusta os coeficientes e torna a superfície de erros mais \"tratável\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando Padronização\n",
    "standardization = StandardScaler()\n",
    "Xst = standardization.fit_transform(X)\n",
    "original_means = standardization.mean_\n",
    "originanal_stds = standardization.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerando X e Y\n",
    "Xst = np.column_stack((Xst,np.ones(observations)))\n",
    "y  = dataset['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def random_w( p ):\n",
    "    return np.array([np.random.normal() for j in range(p)])\n",
    "\n",
    "def hypothesis(X,w):\n",
    "    return np.dot(X,w)\n",
    "\n",
    "def loss(X,w,y):\n",
    "    return hypothesis(X,w) - y\n",
    "\n",
    "def squared_loss(X,w,y):\n",
    "    return loss(X,w,y)**2\n",
    "\n",
    "def gradient(X,w,y):\n",
    "    gradients = list()\n",
    "    n = float(len( y ))\n",
    "    for j in range(len(w)):\n",
    "        gradients.append(np.sum(loss(X,w,y) * X[:,j]) / n)\n",
    "    return gradients\n",
    "\n",
    "def update(X,w,y, alpha = 0.01):\n",
    "    return [t - alpha*g for t, g in zip(w, gradient(X,w,y))]\n",
    "\n",
    "def optimize(X,y, alpha = 0.01, eta = 10**-12, iterations = 1000):\n",
    "    w = random_w(X.shape[1])\n",
    "    path = list()\n",
    "    for k in range(iterations):\n",
    "        SSL = np.sum(squared_loss(X,w,y))\n",
    "        new_w = update(X,w,y, alpha = alpha)\n",
    "        new_SSL = np.sum(squared_loss(X,new_w,y))\n",
    "        w = new_w\n",
    "        if k>=5 and (new_SSL - SSL <= eta and new_SSL - SSL >= -eta):\n",
    "            path.append(new_SSL)\n",
    "            return w, path\n",
    "        if k % (iterations / 20) == 0:\n",
    "            path.append(new_SSL)\n",
    "    return w, path                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeficientes finais padronizados: -0.9204, 1.0810, 0.1430, 0.6822, -2.0601, 2.6706, 0.0211, -3.1044, 2.6588, -2.0759, -2.0622, 0.8566, -3.7487, 22.5328\n"
     ]
    }
   ],
   "source": [
    "# Imprimindo o resultado                           \n",
    "alpha = 0.01\n",
    "w, path = optimize(Xst, y, alpha, eta = 10**-12, iterations = 20000)\n",
    "print (\"Coeficientes finais padronizados: \" + ', '.join(map(lambda x: \"%0.4f\" % x, w)))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desfazendo a Padronização\n",
    "unstandardized_betas = w[:-1] / originanal_stds\n",
    "unstandardized_bias  = w[-1]-np.sum((original_means / originanal_stds) * w[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    bias:  36.4911\n",
      "    CRIM:  -0.1072\n",
      "      ZN:   0.0464\n",
      "   INDUS:   0.0209\n",
      "    CHAS:   2.6886\n",
      "     NOX: -17.7958\n",
      "      RM:   3.8048\n",
      "     AGE:   0.0008\n",
      "     DIS:  -1.4758\n",
      "     RAD:   0.3057\n",
      "     TAX:  -0.0123\n",
      " PTRATIO:  -0.9535\n",
      "       B:   0.0094\n",
      "   LSTAT:  -0.5255\n"
     ]
    }
   ],
   "source": [
    "# Imprimindo o resultado\n",
    "print ('%8s: %8.4f' % ('bias', unstandardized_bias))\n",
    "for beta,varname in zip(unstandardized_betas, variables):\n",
    "    print ('%8s: %8.4f' % (varname, beta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importância dos Atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um modelo\n",
    "modelo = linear_model.LinearRegression(normalize = False, fit_intercept = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.796 NOX\n",
      " 3.805 RM\n",
      " 2.689 CHAS\n",
      " 1.476 DIS\n",
      " 0.953 PTRATIO\n",
      " 0.525 LSTAT\n",
      " 0.306 RAD\n",
      " 0.107 CRIM\n",
      " 0.046 ZN\n",
      " 0.021 INDUS\n",
      " 0.012 TAX\n",
      " 0.009 B\n",
      " 0.001 AGE\n"
     ]
    }
   ],
   "source": [
    "modelo.fit(X,y)\n",
    "for coef, var in sorted(zip(map(abs, modelo.coef_), dataset.columns[:-1]), reverse = True):\n",
    "    print (\"%6.3f %s\" % (coef,var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardization = StandardScaler()\n",
    "Stand_coef_linear_reg = make_pipeline(standardization, modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3.749 LSTAT\n",
      " 3.104 DIS\n",
      " 2.671 RM\n",
      " 2.659 RAD\n",
      " 2.076 TAX\n",
      " 2.062 PTRATIO\n",
      " 2.060 NOX\n",
      " 1.081 ZN\n",
      " 0.920 CRIM\n",
      " 0.857 B\n",
      " 0.682 CHAS\n",
      " 0.143 INDUS\n",
      " 0.021 AGE\n"
     ]
    }
   ],
   "source": [
    "Stand_coef_linear_reg.fit(X,y)\n",
    "for coef, var in sorted(zip(map(abs, Stand_coef_linear_reg.steps[1][1].coef_), dataset.columns[:-1]), reverse = True):\n",
    "    print (\"%6.3f %s\" % (coef,var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando o R Squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = linear_model.LinearRegression(normalize = False, fit_intercept = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_est(X,y):\n",
    "    return r2_score(y, modelo.fit(X,y).predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline R2: 0.741\n"
     ]
    }
   ],
   "source": [
    "print ('Baseline R2: %0.3f' %  r2_est(X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.057 LSTAT\n",
      " 0.044 RM\n",
      " 0.029 DIS\n",
      " 0.028 PTRATIO\n",
      " 0.011 NOX\n",
      " 0.011 RAD\n",
      " 0.006 B\n",
      " 0.006 ZN\n",
      " 0.006 TAX\n",
      " 0.006 CRIM\n",
      " 0.005 CHAS\n",
      " 0.000 INDUS\n",
      " 0.000 AGE\n"
     ]
    }
   ],
   "source": [
    "# Gera o impacto de cada atributo no R2\n",
    "r2_impact = list()\n",
    "for j in range(X.shape[1]):\n",
    "    selection = [i for i in range(X.shape[1]) if i!=j]\n",
    "    r2_impact.append(((r2_est(X,y) - r2_est(X.values[:,selection],y)), dataset.columns[j]))\n",
    "    \n",
    "for imp, varname in sorted(r2_impact, reverse = True):\n",
    "    print ('%6.3f %s' %  (imp, varname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obrigado - Data Science Academy - <a href=\"http://facebook.com/dsacademybr\">facebook.com/dsacademybr</a>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
